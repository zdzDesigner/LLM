import OpenAI from "openai";
import dotenv from "dotenv";
import path from "path";
import { fileURLToPath } from "url";

const openai_api_key = process.env.OPENAI_API_KEY;
const openai_base_url = process.env.OPENAI_BASE_URL;
const openai_model = process.env.OPENAI_MODEL;

// å…¼å®¹ ESM æ¨¡å¼ä¸‹çš„ __dirname
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// åŠ è½½ .env (å°½é‡åœ¨å…¥å£å¤„åŠ è½½ï¼Œä½†è¿™é‡ŒåŠ è½½ä¹Ÿèƒ½ä¿è¯è¿è¡Œ)
dotenv.config({ path: path.resolve(process.cwd(), ".env") });

const client = new OpenAI({
  apiKey: openai_api_key || "dummy-key", // é˜²æ­¢æœªé…ç½® key å¯¼è‡´ç›´æ¥æŠ¥é”™
  baseURL: openai_base_url,
});

export async function callLLM(systemPrompt, userPrompt) {
  // ç®€å•çš„æ ¡éªŒ
  if (!openai_api_key || openai_api_key === "dummy-key") {
    throw new Error("âŒ é”™è¯¯ï¼šæœªåœ¨ .env æ–‡ä»¶ä¸­é…ç½® OPENAI_API_KEY");
  }

  try {
    console.log("ğŸ¤– æ­£åœ¨è¯·æ±‚ AI æ¨¡å‹...");
    const response = await client.chat.completions.create({
      model: openai_model, // å»ºè®®å…ˆç”¨ä¾¿å®œçš„æ¨¡å‹æµ‹è¯•ï¼Œå¦‚ gpt-3.5-turbo æˆ– gpt-4o-mini
      messages: [
        { role: "system", content: systemPrompt },
        { role: "user", content: userPrompt },
      ],
      temperature: 0.7,
    });
    return response.choices[0].message.content;
  } catch (error) {
    console.error("âŒ LLM API è°ƒç”¨å¤±è´¥:", error.message);
    if (error.status === 401) console.error("æç¤ºï¼šè¯·æ£€æŸ¥ API Key æ˜¯å¦æ­£ç¡®ã€‚");
    throw error;
  }
}
